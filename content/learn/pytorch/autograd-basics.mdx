---
title: অটোগ্রাড পরিচিতি - গ্রেডিয়েন্ট ট্র্যাকিং
icon: solar:alt-arrow-right-bold-duotone
---

## অটোগ্রাড কী?

PyTorch-এর অটোগ্রাড (Autograd) হচ্ছে একটি অটোমেটিক ডিফারেনশিয়েশন সিস্টেম, যা নিউরাল নেটওয়ার্ক ট্রেইনিংয়ের সময় গ্রেডিয়েন্ট হিসাব করে দেয়। গ্রেডিয়েন্ট মানে—কোনো ফাংশনের ইনপুট পরিবর্তন করলে আউটপুট কতটা পরিবর্তন হয়।

_অ্যানালজি:_ অটোগ্রাডকে ভাবুন একজন হিসাবরক্ষক হিসেবে—যিনি প্রতিটি পরিবর্তনের হিসাব রাখেন, যাতে আপনি সহজে বুঝতে পারেন কোন পথে গেলে লাভ বেশি হবে।

## কেন গ্রেডিয়েন্ট দরকার?

নিউরাল নেটওয়ার্ক ট্রেইনিংয়ে মূল লক্ষ্য হলো—লস (error) কমানো। এজন্য গ্রেডিয়েন্ট ব্যবহার করে ওজন (weights) আপডেট করা হয়।

## PyTorch-এ অটোগ্রাড কিভাবে কাজ করে?

PyTorch-এ প্রতিটি টেনসর আছে `.requires_grad` অ্যাট্রিবিউট। যদি এটি `True` করা হয়, তাহলে সেই টেনসরের উপর যত অপারেশন হবে, তার হিসাব PyTorch রাখবে।

### উদাহরণ:
```python
import torch
x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x ** 2 + 5 * x
z = y.sum()
z.backward()  # গ্রেডিয়েন্ট হিসাব
print(x.grad)  # tensor([9., 11.])
```

এখানে `x.grad`-এ x-এর প্রতিটি ভ্যালুর জন্য গ্রেডিয়েন্ট পাওয়া যাবে।

## Computational Graph

অটোগ্রাড প্রতিটি অপারেশনকে একটি গ্রাফে সংরক্ষণ করে—যাকে computational graph বলে। ব্যাকপ্রপাগেশন চলার সময় এই গ্রাফ ব্যবহার করে গ্রেডিয়েন্ট হিসাব করা হয়।

_অ্যানালজি:_ এটি একটি ম্যাপের মতো—যেখানে প্রতিটি রাস্তা ধরে PyTorch হিসাব করে কোন পথে কত পরিবর্তন হচ্ছে।

## requires_grad ও no_grad

- `.requires_grad=True`: গ্রেডিয়েন্ট ট্র্যাক হবে
- `with torch.no_grad()`: এই ব্লকের মধ্যে কোনো গ্রেডিয়েন্ট ট্র্যাক হবে না (ইনফারেন্সের সময় দরকার)

```python
with torch.no_grad():
    y = x * 2  # এখানে গ্রেডিয়েন্ট ট্র্যাক হবে না
```

## রিয়েল-ওয়ার্ল্ড উদাহরণ

ধরা যাক, আপনি একটি ইমেজ ক্লাসিফিকেশন মডেল ট্রেইন করছেন। প্রতিটি ব্যাচে লস কমানোর জন্য PyTorch অটোগ্রাড দিয়ে ওজন আপডেট করে।

## কমন ফাংশন
- `.backward()`: গ্রেডিয়েন্ট হিসাব
- `.grad`: গ্রেডিয়েন্ট অ্যাক্সেস
- `.zero_()`: গ্রেডিয়েন্ট রিসেট

```python
optimizer.zero_grad()  # প্রতি ইটারেশনে গ্রেডিয়েন্ট রিসেট
```

## সতর্কতা
- গ্রেডিয়েন্ট অ্যাকুমুলেট হয়—প্রতি ইটারেশনে `.zero_()` দিয়ে রিসেট করুন
- ইনফারেন্সের সময় `no_grad` ব্যবহার করুন—মেমরি ও স্পিড বাড়ে

## সংক্ষিপ্ত চেকলিস্ট
- requires_grad=True দিয়ে টেনসর তৈরি
- অপারেশন শেষে .backward() কল
- x.grad দিয়ে গ্রেডিয়েন্ট দেখুন
- optimizer.zero_grad() দিয়ে রিসেট

## উপসংহার

অটোগ্রাড PyTorch-এর সবচেয়ে শক্তিশালী ফিচার—এটি নিউরাল নেটওয়ার্ক ট্রেইনিংকে সহজ, দ্রুত ও নির্ভুল করে তোলে। গ্রেডিয়েন্ট ট্র্যাকিং বুঝলে ডিপ লার্নিংয়ের মূল গাণিতিক ধারণা পরিষ্কার হবে।

_পরবর্তী পাঠে: GPU এক্সিলারেশন—CUDA টেনসর ব্যবহার!_
