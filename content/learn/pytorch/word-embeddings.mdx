---
title: ওয়ার্ড এমবেডিংস (Word2Vec, GloVe)
icon: solar:alt-arrow-right-bold-duotone
---

প্রতিদিন আমরা অসংখ্য শব্দ ব্যবহার করি, কিন্তু কম্পিউটার কীভাবে "শব্দ" বোঝে? ধরুন, "বই" আর "খাতা"—মানুষ জানে এগুলো কাছাকাছি অর্থের, কিন্তু কম্পিউটারের কাছে তো এগুলো শুধু কিছু অক্ষর! এই সমস্যার সমাধানই হলো **ওয়ার্ড এমবেডিংস**।

## ওয়ার্ড এমবেডিংস কী?

ওয়ার্ড এমবেডিংস হচ্ছে এমন এক টেকনিক, যেখানে প্রতিটি শব্দকে একটি সংখ্যার ভেক্টর (vector) হিসেবে প্রকাশ করা হয়। এই ভেক্টরগুলো এমনভাবে তৈরি হয়, যাতে অর্থের দিক থেকে কাছাকাছি শব্দের ভেক্টরও কাছাকাছি থাকে।

### বাস্তব উদাহরণ

ধরা যাক, "রাজধানী" শব্দের ভেক্টর আর "শহর" শব্দের ভেক্টর—এরা কাছাকাছি থাকবে, কারণ অর্থে মিল আছে। আবার "বই" আর "কম্পিউটার"—এদের ভেক্টর দূরে থাকবে।

## কেন দরকার?

কম্পিউটার যদি শুধু অক্ষর দেখে, তাহলে "বই" আর "খাতা"-র মধ্যে কোনো সম্পর্ক খুঁজে পাবে না। কিন্তু এমবেডিংস ব্যবহার করলে, মেশিন লার্নিং মডেল সহজেই বুঝতে পারে কোন শব্দগুলো একে অপরের সাথে সম্পর্কিত।

## ওয়ার্ড এমবেডিংস কীভাবে কাজ করে?

প্রতিটি শব্দকে একটি নির্দিষ্ট দৈর্ঘ্যের ভেক্টর হিসেবে প্রকাশ করা হয় (যেমন, 100 বা 300 dimension)। এই ভেক্টরগুলো ট্রেনিং ডেটা থেকে শেখা হয়, যাতে অর্থের মিল/ভিন্নতা ধরা পড়ে।

## জনপ্রিয় ওয়ার্ড এমবেডিংস: Word2Vec ও GloVe

### Word2Vec

Word2Vec হলো গুগলের তৈরি একটি মডেল, যা "শব্দের আশেপাশের শব্দ" দেখে ভেক্টর শেখে। ধরুন, "বাংলাদেশের রাজধানী ঢাকা"—এখানে "রাজধানী" শব্দের আশেপাশে "বাংলাদেশ" ও "ঢাকা" আছে। Word2Vec এই তথ্য ব্যবহার করে ভেক্টর তৈরি করে।

Word2Vec-এর দুইটি প্রধান পদ্ধতি:
- **CBOW (Continuous Bag of Words):** আশেপাশের শব্দ দেখে মাঝের শব্দ অনুমান করে।
- **Skip-gram:** মাঝের শব্দ দেখে আশেপাশের শব্দ অনুমান করে।

### GloVe

GloVe (Global Vectors for Word Representation) হলো স্ট্যানফোর্ডের তৈরি একটি টেকনিক। এটি পুরো ডেটাসেটের "co-occurrence" (একসাথে কতবার এসেছে) দেখে ভেক্টর তৈরি করে।

## বাস্তব জীবনের অ্যানালজি

ধরা যাক, আপনি একটি শহরে থাকেন। আপনার আশেপাশে যারা বেশি থাকে, তাদের সাথে আপনার সম্পর্ক বেশি। Word2Vec-ও ঠিক এমন—শব্দের "পড়শি" দেখে সম্পর্ক বোঝে।

GloVe-কে ভাবুন, পুরো শহরের মানুষের মধ্যে কে কার সাথে কতবার দেখা করেছে, সেই হিসাব রেখে সম্পর্ক বোঝে।

## বাস্তব জীবনের analogy: বন্ধুত্বের গ্রুপ

যেমন, স্কুলে যারা একসাথে বেশি সময় কাটায়, তাদের মধ্যে বন্ধুত্ব বেশি। তেমনি, শব্দগুলোও যেসব টেক্সটে একসাথে বেশি আসে, তাদের ভেক্টরও কাছাকাছি হয়।

## PyTorch-এ ওয়ার্ড এমবেডিংস

PyTorch-এ ওয়ার্ড এমবেডিংস তৈরি করা খুব সহজ।

```python
import torch
import torch.nn as nn

# ধরুন, আমাদের vocabulary size 1000, embedding dimension 300
embedding = nn.Embedding(num_embeddings=1000, embedding_dim=300)

# একটি শব্দের index (যেমন, 42)
word_idx = torch.LongTensor([42])

# এমবেডিং ভেক্টর বের করা
word_vec = embedding(word_idx)
print(word_vec.shape)  # (1, 300)
```

### প্রি-ট্রেইনড এমবেডিংস ব্যবহার

Word2Vec বা GloVe-এর প্রি-ট্রেইনড ভেক্টর ডাউনলোড করে PyTorch-এ ব্যবহার করা যায়।

```python
import numpy as np
import torch
import torch.nn as nn

# GloVe থেকে লোড করা এমবেডিং ম্যাট্রিক্স (numpy array)
glove_matrix = np.load('glove.npy')  # (vocab_size, embedding_dim)

# PyTorch-এ কনভার্ট
weights = torch.FloatTensor(glove_matrix)
embedding = nn.Embedding.from_pretrained(weights)

# শব্দের index দিয়ে ভেক্টর বের করা
word_idx = torch.LongTensor([42])
word_vec = embedding(word_idx)
```

## ওয়ার্ড এমবেডিংস দিয়ে কী করা যায়?
- টেক্সট ক্লাসিফিকেশন
- সেন্টিমেন্ট অ্যানালাইসিস
- মেশিন ট্রান্সলেশন
- চ্যাটবট

## চ্যালেঞ্জ ও সীমাবদ্ধতা
- Out-of-vocabulary (OOV): নতুন শব্দ থাকলে এমবেডিংস নেই
- Context বোঝে না: "bank" মানে নদীর পাড় না ব্যাংক?

## আধুনিক এমবেডিংস

Word2Vec ও GloVe-statik, অর্থাৎ context বোঝে না। আধুনিক NLP-তে BERT, GPT-এর মতো context-aware এমবেডিংস ব্যবহৃত হয়।

## উপসংহার

ওয়ার্ড এমবেডিংস NLP-র ভিত্তি। এগুলো ছাড়া টেক্সট ডেটা নিয়ে কাজ করা কঠিন। Word2Vec ও GloVe সহজ, শক্তিশালী, এবং PyTorch-এ সহজেই ব্যবহারযোগ্য।

## সংক্ষেপে
ওয়ার্ড এমবেডিংস NLP-র ভিত্তি, যা শব্দকে অর্থপূর্ণ সংখ্যায় রূপান্তর করে। Word2Vec ও GloVe দুইটি জনপ্রিয় টেকনিক, এবং PyTorch-এ সহজেই ব্যবহার করা যায়।

---

## অনুশীলন
- আপনার প্রিয় ৫টি শব্দের এমবেডিং বের করুন
- "রাজধানী" ও "শহর" শব্দের এমবেডিং তুলনা করুন
- প্রি-ট্রেইনড GloVe এমবেডিং দিয়ে টেক্সট ক্লাসিফিকেশন শুরু করুন

---

## আরও পড়ুন
- [Word2Vec Explained](https://jalammar.github.io/illustrated-word2vec/)
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
- [PyTorch Embedding Docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)

পরবর্তী পাঠে, আমরা RNN, LSTM, GRU নিয়ে আলোচনা করবো—যেখানে এমবেডিংসের ব্যবহার আরও গভীরভাবে দেখা যাবে।
